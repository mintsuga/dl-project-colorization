{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import skimage\n",
    "from skimage.color import rgb2gray\n",
    "import keras\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CifarGenerator(keras.utils.Sequence):\n",
    "    \"\"\"\n",
    "    a generator of Cifar dataset\n",
    "    use 'for' iteration to generate image in batch-manner\n",
    "    \"\"\"\n",
    "    def __init__(self, img_dir, batch_size, is_training=True, shuffle=True):\n",
    "        self.img_dir = img_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self._is_training = is_training\n",
    "\n",
    "        self._img_names = []\n",
    "\n",
    "        self.indexes = np.arange(len(self.img_names))\n",
    "\n",
    "        self.w = 32\n",
    "        self.h = 32\n",
    "\n",
    "    def __len__(self):\n",
    "        # how many batches \n",
    "        return int(np.ceil(len(self.indexes) / float(self.batch_size)))\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.is_training and self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        upper_bound = min(self.size, (index + 1) * self.batch_size)\n",
    "        indexes = self.indexes[index * self.batch_size:upper_bound]\n",
    "\n",
    "        images_A = []\n",
    "        images_B = []\n",
    "        f_names = []\n",
    "        \n",
    "        for index in indexes:\n",
    "            f_name = self._img_names[index]\n",
    "            f_names.append(os.path.split(f_name)[1])\n",
    "            \n",
    "            image_A = scipy.misc.imread(f_name, mode='RGB').astype(np.float)\n",
    "            image_B = rgb2gray(image_A)\n",
    "        \n",
    "            image_A = scipy.misc.imresize(image_A, (self.h, self.w))\n",
    "            image_B = scipy.misc.imresize(image_B, (self.h, self.w))\n",
    "            \n",
    "            # convert gray-scale image to a 3-channel image to fit input shape\n",
    "            image_B = np.stack((image_B,)*3, axis=-1)\n",
    "                \n",
    "            images_A.append(image_A)\n",
    "            images_B.append(image_B)\n",
    "        \n",
    "        # normalization is to bring the values in range [-1.0,1.0]\n",
    "        images_A = np.array(images_A)/127.5 - 1.\n",
    "        images_B = np.array(images_B)/127.5 - 1.\n",
    "        \n",
    "        if self.is_training:\n",
    "            return images_A, images_B\n",
    "        else:\n",
    "            return images_B, f_names\n",
    "\n",
    "    def _load(self):\n",
    "        for f_name in tqdm(os.listdir(self.img_dir)):\n",
    "            if os.path.splitext(f_name)[-1] == '.png':\n",
    "                self._img_names.append(os.path.join(self.img_dir, f_name))\n",
    "\n",
    "    @property\n",
    "    def img_names(self):\n",
    "        if len(self._img_names) == 0:\n",
    "            self._load()\n",
    "        return self._img_names\n",
    "\n",
    "    @property\n",
    "    def size(self):\n",
    "        return len(self._img_names)\n",
    "\n",
    "    @property\n",
    "    def is_training(self):\n",
    "        return self._is_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = '/data_service/source_datasets/cifar_images/images_train'\n",
    "TEST_DIR = '/data_service/source_datasets/cifar_images/images_test'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pix2pixColorizer():\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.h = 32\n",
    "        self.w = 32\n",
    "        self.num_channel = 3\n",
    "        self.image_shape = (self.h, self.w, self.num_channel)\n",
    "        \n",
    "        self.image_A = Input(shape=self.image_shape)\n",
    "        self.image_B = Input(shape=self.image_shape)\n",
    "        \n",
    "        self.build_gan()\n",
    "    \n",
    "    def load_generator(self, g_model):\n",
    "        self.generator.load_weights(g_model)\n",
    "    \n",
    "    def build_gan(self):\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='mse', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])\n",
    "        \n",
    "        self.generator = self.build_generator()\n",
    "        self.fake_A = self.generator(self.image_B)\n",
    "        self.discriminator.trainable = False\n",
    "        \n",
    "        self.valid = self.discriminator([self.fake_A, self.image_B])\n",
    "        self.combined = Model(inputs=[self.image_A, self.image_B], outputs=[self.valid, self.fake_A])\n",
    "        self.combined.compile(loss=['mse', 'mae'], loss_weights=[1, 100], optimizer=Adam(0.0002, 0.5))\n",
    "        \n",
    "        self.disc_patch = (int(self.h/2**4), int(self.w/2**4), 1)\n",
    "    \n",
    "    def build_discriminator(self):\n",
    "        image_A = Input(shape=self.image_shape)\n",
    "        image_B = Input(shape=self.image_shape)\n",
    "        \n",
    "        # concat axis=channel axis\n",
    "        combined_images = Concatenate(axis=-1)([image_A, image_B])\n",
    "        \n",
    "        # layer 1\n",
    "        d1 = Conv2D(filters=64, kernel_size=4, strides=2, padding='same')(combined_images)\n",
    "        d1 = LeakyReLU(alpha=0.2)(d1)\n",
    "        \n",
    "        # layer 2\n",
    "        d2 = Conv2D(filters=128, kernel_size=4, strides=2, padding='same')(d1)\n",
    "        d2 = LeakyReLU(alpha=0.2)(d2)\n",
    "        d2 = BatchNormalization(momentum=0.8)(d2)\n",
    "        \n",
    "        # layer 3\n",
    "        d3 = Conv2D(filters=128, kernel_size=4, strides=2, padding='same')(d2)\n",
    "        d3 = LeakyReLU(alpha=0.2)(d3)\n",
    "        d3 = BatchNormalization(momentum=0.8)(d3)\n",
    "        \n",
    "        # layer 4\n",
    "        d4 = Conv2D(filters=128, kernel_size=4, strides=2, padding='same')(d3)\n",
    "        d4 = LeakyReLU(alpha=0.2)(d4)\n",
    "        d4 = BatchNormalization(momentum=0.8)(d4)\n",
    "        \n",
    "        # 1-dim output\n",
    "        validity = Conv2D(1, kernel_size=4, strides=1, padding='same')(d4)\n",
    "        \n",
    "        return Model([image_A, image_B], validity)\n",
    "    \n",
    "    def build_generator(self):\n",
    "        \n",
    "        d0 = Input(shape=self.image_shape)\n",
    "        \n",
    "        # layer 1\n",
    "        d1 = Conv2D(filters=64, kernel_size=4, strides=2, padding='same')(d0)\n",
    "        d1 = LeakyReLU(alpha=0.2)(d1)\n",
    "        \n",
    "        # layer 2\n",
    "        d2 = Conv2D(filters=128, kernel_size=4, strides=2, padding='same')(d1)\n",
    "        d2 = LeakyReLU(alpha=0.2)(d2)\n",
    "        d2 = BatchNormalization(momentum=0.8)(d2)\n",
    "        \n",
    "        # layer 3\n",
    "        d3 = Conv2D(filters=256, kernel_size=4, strides=2, padding='same')(d2)\n",
    "        d3 = LeakyReLU(alpha=0.2)(d3)\n",
    "        d3 = BatchNormalization(momentum=0.8)(d3)\n",
    "        \n",
    "        # layer 4\n",
    "        d4 = Conv2D(filters=512, kernel_size=4, strides=2, padding='same')(d3)\n",
    "        d4 = LeakyReLU(alpha=0.2)(d4)\n",
    "        d4 = BatchNormalization(momentum=0.8)(d4)\n",
    "        \n",
    "        # layer 5\n",
    "        d5 = Conv2D(filters=512, kernel_size=4, strides=2, padding='same')(d4)\n",
    "        d5 = LeakyReLU(alpha=0.2)(d5)\n",
    "        d5 = BatchNormalization(momentum=0.8)(d5)\n",
    "        \n",
    "        # layer 4\n",
    "        u4 = UpSampling2D(size=2)(d5)\n",
    "        u4 = Conv2D(filters=512, kernel_size=4, strides=1, padding='same', activation='relu')(u4)\n",
    "        u4 = BatchNormalization(momentum=0.8)(u4)\n",
    "        u4 = Concatenate()([u4, d4])\n",
    "        \n",
    "        # layer 3\n",
    "        u3 = UpSampling2D(size=2)(u4)\n",
    "        u3 = Conv2D(filters=256, kernel_size=4, strides=1, padding='same', activation='relu')(u3)\n",
    "        u3 = BatchNormalization(momentum=0.8)(u3)\n",
    "        u3 = Concatenate()([u3, d3])\n",
    "        \n",
    "        # layer 2\n",
    "        u2 = UpSampling2D(size=2)(u3)\n",
    "        u2 = Conv2D(filters=128, kernel_size=4, strides=1, padding='same', activation='relu')(u2)\n",
    "        u2 = BatchNormalization(momentum=0.8)(u2)\n",
    "        u2 = Concatenate()([u2, d2])\n",
    "        \n",
    "        # layer 1\n",
    "        u1 = UpSampling2D(size=2)(u2)\n",
    "        u1 = Conv2D(filters=64, kernel_size=4, strides=1, padding='same', activation='relu')(u1)\n",
    "        u1 = BatchNormalization(momentum=0.8)(u1)\n",
    "        u1 = Concatenate()([u1, d1])\n",
    "        \n",
    "        # layer 0\n",
    "        u0 = UpSampling2D(size=2)(u1)\n",
    "        \n",
    "        # 3-dim output\n",
    "        u0 = Conv2D(self.num_channel, kernel_size=4, strides=1, padding='same', activation='tanh')(u0)\n",
    "        \n",
    "        return Model(d0, u0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "colorizer = pix2pixColorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend.tensorflow_backend as KTF\n",
    "import tensorflow as tf\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\"\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "KTF.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "SAVED_DIR = './samples'\n",
    "\n",
    "def save_single(save_dir, f_name, image):\n",
    "    image = np.uint8((np.array(image) * 0.5 + 0.5) * 255)\n",
    "    image = Image.fromarray(image)\n",
    "    image.save(save_dir + '/' + f_name, quality=95)\n",
    "    \n",
    "def save_intermediate_images(save_dir, batch_i, images_A, images_B, fake_images_A):\n",
    "    assert images_A.shape[0] == images_B.shape[0] == fake_images_A.shape[0]\n",
    "    \n",
    "    batch_size = images_A.shape[0]\n",
    "    generated_image = Image.new('RGB', (32*3, 32*batch_size))\n",
    "    for batch_cnt in range(batch_size):\n",
    "        image_A = np.uint8((np.array(images_A[batch_cnt]) * 0.5 + 0.5) * 255)\n",
    "        image_B = np.uint8((np.array(images_B[batch_cnt]) * 0.5 + 0.5) * 255)\n",
    "        image_fake_A = np.uint8((np.array(fake_A[batch_cnt]) * 0.5 + 0.5) * 255)\n",
    "        \n",
    "        image_A = Image.fromarray(image_A)\n",
    "        image_B = Image.fromarray(image_B)\n",
    "        image_fake_A = Image.fromarray(image_fake_A)\n",
    "        \n",
    "        generated_image.paste(image_B,      (0, batch_cnt*32, 32, (batch_cnt+1)*32))\n",
    "        generated_image.paste(image_fake_A, (32, batch_cnt*32, 32*2, (batch_cnt+1)*32))\n",
    "        generated_image.paste(image_A,      (32*2, batch_cnt*32, 32*3, (batch_cnt+1)*32))\n",
    "    \n",
    "    generated_image.save(save_dir + \"/G_%d.jpg\" % batch_i, quality=95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "def train():\n",
    "    \n",
    "    # set up tensorboard logs\n",
    "    writer_1 = tf.summary.FileWriter(\"./logs/g_loss_50\")\n",
    "    writer_2 = tf.summary.FileWriter(\"./logs/d_loss_50\")\n",
    "         \n",
    "    loss_var = tf.Variable(0.0)\n",
    "    tf.summary.scalar(\"loss\", loss_var)\n",
    "         \n",
    "    write_op = tf.summary.merge_all()\n",
    "    \n",
    "    # set hyper-params\n",
    "    n_epoch = 50\n",
    "    batch_size = 64\n",
    "    \n",
    "    data_generator = CifarGenerator(img_dir=TRAIN_DIR, batch_size=batch_size, is_training=True)\n",
    "    test_generator = CifarGenerator(img_dir=TEST_DIR, batch_size=batch_size)\n",
    "\n",
    "    valid = np.ones((batch_size,) + colorizer.disc_patch)\n",
    "    fake = np.zeros((batch_size,) + colorizer.disc_patch)\n",
    "    \n",
    "    for epoch_cnt in range(n_epoch):\n",
    "        print('Epoch %d: ' % epoch_cnt)\n",
    "        \n",
    "        valid = np.ones((batch_size,) + colorizer.disc_patch)\n",
    "        fake = np.zeros((batch_size,) + colorizer.disc_patch)\n",
    "\n",
    "        for batch_cnt, (images_A, images_B) in enumerate(tqdm(data_generator)):\n",
    "            total_cnt = epoch_cnt * len(data_generator) + batch_cnt\n",
    "            \n",
    "            # last batch, less samples\n",
    "            if images_A.shape[0] != batch_size:\n",
    "                valid = np.ones((images_A.shape[0],) + colorizer.disc_patch)\n",
    "                fake = np.zeros((images_A.shape[0],) + colorizer.disc_patch)\n",
    "\n",
    "            fake_A = colorizer.generator.predict(images_B)\n",
    "\n",
    "            d_loss_real = colorizer.discriminator.train_on_batch([images_A, images_B], valid)\n",
    "            d_loss_fake = colorizer.discriminator.train_on_batch([fake_A, images_B], fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            g_loss = colorizer.combined.train_on_batch([images_A, images_B], [valid, images_A])\n",
    "            \n",
    "            # write loss for each step\n",
    "            summary = sess.run(write_op, {loss_var: g_loss[0]})\n",
    "            writer_1.add_summary(summary, total_cnt)\n",
    "            writer_1.flush()\n",
    "             \n",
    "\n",
    "            summary = sess.run(write_op, {loss_var: d_loss[0]})\n",
    "            writer_2.add_summary(summary, total_cnt)\n",
    "            writer_2.flush()\n",
    "\n",
    "            if batch_cnt and not batch_cnt % 200:\n",
    "                print (\"[Batch %d/%d] [D loss: %f, acc: %3d%%] [G loss: %f]\" % \n",
    "                               (batch_cnt+1, len(generator), \n",
    "                                d_loss[0], 100*d_loss[1], g_loss[0]))\n",
    "                test_A, test_B = test_generator[0]\n",
    "                fake_A = colorizer.generator.predict(test_B)\n",
    "                save_intermediate_images(SAVED_DIR, batch_cnt, test_A, test_B, fake_A)\n",
    "\n",
    "    colorizer.generator.save_weights('g_weights.h5')\n",
    "    colorizer.discriminator.save_weights('d_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test process, save gray/generated/original\n",
    "# batch_size = 16\n",
    "# colorizer.load_generator('g_weights_50.h5')  # generator params with 20-epoch training\n",
    "# data_generator = CifarGenerator(img_dir=TEST_DIR, batch_size=batch_size)\n",
    "\n",
    "# SAVED_DIR = './cGAN_50_Res'\n",
    "# for batch_cnt, (images_A, images_B) in enumerate(tqdm(data_generator)):\n",
    "#     fake_A = colorizer.generator.predict(images_B)\n",
    "#     save_intermediate_images(SAVED_DIR, batch_cnt, images_A, images_B, fake_A)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/10000 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 212608.81it/s]\u001b[A\n",
      "  0%|          | 0/2500 [00:00<?, ?it/s]\u001b[A/usr/local/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:39: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/usr/local/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:42: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "/usr/local/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:43: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "\n",
      "  1%|          | 20/2500 [00:00<00:12, 194.40it/s]\u001b[A\n",
      "  2%|▏         | 43/2500 [00:00<00:12, 202.23it/s]\u001b[A\n",
      "  3%|▎         | 65/2500 [00:00<00:11, 207.22it/s]\u001b[A\n",
      "  3%|▎         | 84/2500 [00:00<00:12, 199.46it/s]\u001b[A\n",
      "  4%|▍         | 102/2500 [00:00<00:12, 191.04it/s]\u001b[A\n",
      "  5%|▍         | 122/2500 [00:00<00:12, 192.92it/s]\u001b[A\n",
      "  6%|▌         | 143/2500 [00:00<00:12, 195.22it/s]\u001b[A\n",
      "  7%|▋         | 163/2500 [00:00<00:12, 194.20it/s]\u001b[A\n",
      "  7%|▋         | 182/2500 [00:00<00:12, 190.33it/s]\u001b[A\n",
      "  8%|▊         | 203/2500 [00:01<00:11, 193.26it/s]\u001b[A\n",
      "  9%|▉         | 225/2500 [00:01<00:11, 198.74it/s]\u001b[A\n",
      " 10%|▉         | 247/2500 [00:01<00:11, 204.41it/s]\u001b[A\n",
      " 11%|█         | 269/2500 [00:01<00:10, 208.53it/s]\u001b[A\n",
      " 12%|█▏        | 291/2500 [00:01<00:10, 210.11it/s]\u001b[A\n",
      " 13%|█▎        | 315/2500 [00:01<00:10, 217.29it/s]\u001b[A\n",
      " 14%|█▎        | 340/2500 [00:01<00:09, 224.61it/s]\u001b[A\n",
      " 15%|█▍        | 363/2500 [00:01<00:09, 225.09it/s]\u001b[A\n",
      " 15%|█▌        | 386/2500 [00:01<00:09, 225.19it/s]\u001b[A\n",
      " 16%|█▋        | 409/2500 [00:01<00:09, 223.51it/s]\u001b[A\n",
      " 17%|█▋        | 432/2500 [00:02<00:09, 221.77it/s]\u001b[A\n",
      " 18%|█▊        | 456/2500 [00:02<00:09, 224.30it/s]\u001b[A\n",
      " 19%|█▉        | 479/2500 [00:02<00:09, 222.72it/s]\u001b[A\n",
      " 20%|██        | 504/2500 [00:02<00:08, 228.23it/s]\u001b[A\n",
      " 21%|██        | 527/2500 [00:02<00:08, 227.45it/s]\u001b[A\n",
      " 22%|██▏       | 550/2500 [00:02<00:08, 226.57it/s]\u001b[A\n",
      " 23%|██▎       | 573/2500 [00:02<00:08, 226.49it/s]\u001b[A\n",
      " 24%|██▍       | 596/2500 [00:02<00:08, 227.15it/s]\u001b[A\n",
      " 25%|██▍       | 619/2500 [00:02<00:08, 221.08it/s]\u001b[A\n",
      " 26%|██▌       | 642/2500 [00:02<00:08, 223.34it/s]\u001b[A\n",
      " 27%|██▋       | 665/2500 [00:03<00:08, 224.99it/s]\u001b[A\n",
      " 28%|██▊       | 688/2500 [00:03<00:08, 226.20it/s]\u001b[A\n",
      " 28%|██▊       | 712/2500 [00:03<00:07, 228.72it/s]\u001b[A\n",
      " 29%|██▉       | 735/2500 [00:03<00:07, 222.98it/s]\u001b[A\n",
      " 30%|███       | 758/2500 [00:03<00:07, 219.44it/s]\u001b[A\n",
      " 31%|███       | 780/2500 [00:03<00:07, 216.67it/s]\u001b[A\n",
      " 32%|███▏      | 802/2500 [00:03<00:07, 213.95it/s]\u001b[A\n",
      " 33%|███▎      | 824/2500 [00:03<00:07, 212.01it/s]\u001b[A\n",
      " 34%|███▍      | 846/2500 [00:03<00:08, 205.51it/s]\u001b[A\n",
      " 35%|███▍      | 868/2500 [00:04<00:07, 207.73it/s]\u001b[A\n",
      " 36%|███▌      | 890/2500 [00:04<00:07, 211.14it/s]\u001b[A\n",
      " 37%|███▋      | 913/2500 [00:04<00:07, 215.20it/s]\u001b[A\n",
      " 37%|███▋      | 936/2500 [00:04<00:07, 216.80it/s]\u001b[A\n",
      " 38%|███▊      | 958/2500 [00:04<00:07, 212.64it/s]\u001b[A\n",
      " 39%|███▉      | 981/2500 [00:04<00:07, 216.20it/s]\u001b[A\n",
      " 40%|████      | 1005/2500 [00:04<00:06, 220.50it/s]\u001b[A\n",
      " 41%|████      | 1029/2500 [00:04<00:06, 225.03it/s]\u001b[A\n",
      " 42%|████▏     | 1052/2500 [00:04<00:06, 226.21it/s]\u001b[A\n",
      " 43%|████▎     | 1075/2500 [00:04<00:06, 225.36it/s]\u001b[A\n",
      " 44%|████▍     | 1098/2500 [00:05<00:06, 221.53it/s]\u001b[A\n",
      " 45%|████▍     | 1122/2500 [00:05<00:06, 224.45it/s]\u001b[A\n",
      " 46%|████▌     | 1146/2500 [00:05<00:05, 227.61it/s]\u001b[A\n",
      " 47%|████▋     | 1171/2500 [00:05<00:05, 232.46it/s]\u001b[A\n",
      " 48%|████▊     | 1195/2500 [00:05<00:05, 234.54it/s]\u001b[A\n",
      " 49%|████▉     | 1219/2500 [00:05<00:05, 235.32it/s]\u001b[A\n",
      " 50%|████▉     | 1243/2500 [00:05<00:05, 233.74it/s]\u001b[A\n",
      " 51%|█████     | 1267/2500 [00:05<00:06, 183.83it/s]\u001b[A\n",
      " 52%|█████▏    | 1288/2500 [00:06<00:08, 148.78it/s]\u001b[A\n",
      " 52%|█████▏    | 1306/2500 [00:06<00:08, 133.26it/s]\u001b[A\n",
      " 53%|█████▎    | 1322/2500 [00:06<00:09, 122.12it/s]\u001b[A\n",
      " 53%|█████▎    | 1336/2500 [00:06<00:09, 119.28it/s]\u001b[A\n",
      " 54%|█████▍    | 1349/2500 [00:06<00:09, 115.75it/s]\u001b[A\n",
      " 54%|█████▍    | 1362/2500 [00:06<00:10, 113.76it/s]\u001b[A\n",
      " 55%|█████▍    | 1374/2500 [00:06<00:10, 109.82it/s]\u001b[A\n",
      " 56%|█████▌    | 1388/2500 [00:06<00:09, 117.01it/s]\u001b[A\n",
      " 56%|█████▌    | 1404/2500 [00:07<00:08, 126.66it/s]\u001b[A\n",
      " 57%|█████▋    | 1419/2500 [00:07<00:08, 132.08it/s]\u001b[A\n",
      " 57%|█████▋    | 1435/2500 [00:07<00:07, 136.95it/s]\u001b[A\n",
      " 58%|█████▊    | 1450/2500 [00:07<00:07, 137.68it/s]\u001b[A\n",
      " 59%|█████▊    | 1465/2500 [00:07<00:08, 127.42it/s]\u001b[A\n",
      " 59%|█████▉    | 1479/2500 [00:07<00:08, 125.48it/s]\u001b[A\n",
      " 60%|█████▉    | 1492/2500 [00:07<00:08, 122.89it/s]\u001b[A\n",
      " 60%|██████    | 1506/2500 [00:07<00:07, 126.21it/s]\u001b[A\n",
      " 61%|██████    | 1519/2500 [00:07<00:07, 126.37it/s]\u001b[A\n",
      " 61%|██████▏   | 1532/2500 [00:08<00:07, 126.97it/s]\u001b[A\n",
      " 62%|██████▏   | 1545/2500 [00:08<00:07, 121.94it/s]\u001b[A\n",
      " 62%|██████▏   | 1558/2500 [00:08<00:07, 120.89it/s]\u001b[A\n",
      " 63%|██████▎   | 1573/2500 [00:08<00:07, 126.59it/s]\u001b[A\n",
      " 64%|██████▎   | 1588/2500 [00:08<00:06, 130.38it/s]\u001b[A\n",
      " 64%|██████▍   | 1602/2500 [00:08<00:06, 132.21it/s]\u001b[A\n",
      " 65%|██████▍   | 1616/2500 [00:08<00:06, 130.25it/s]\u001b[A\n",
      " 65%|██████▌   | 1630/2500 [00:08<00:07, 124.25it/s]\u001b[A\n",
      " 66%|██████▌   | 1643/2500 [00:08<00:07, 118.04it/s]\u001b[A\n",
      " 66%|██████▌   | 1655/2500 [00:09<00:07, 113.51it/s]\u001b[A\n",
      " 67%|██████▋   | 1667/2500 [00:09<00:07, 114.55it/s]\u001b[A\n",
      " 67%|██████▋   | 1680/2500 [00:09<00:06, 118.73it/s]\u001b[A\n",
      " 68%|██████▊   | 1692/2500 [00:09<00:06, 117.55it/s]\u001b[A\n",
      " 68%|██████▊   | 1704/2500 [00:09<00:06, 116.84it/s]\u001b[A\n",
      " 69%|██████▊   | 1716/2500 [00:09<00:07, 111.75it/s]\u001b[A\n",
      " 69%|██████▉   | 1728/2500 [00:09<00:07, 110.08it/s]\u001b[A\n",
      " 70%|██████▉   | 1740/2500 [00:09<00:06, 109.76it/s]\u001b[A\n",
      " 70%|███████   | 1753/2500 [00:09<00:06, 114.16it/s]\u001b[A\n",
      " 71%|███████   | 1768/2500 [00:10<00:06, 121.33it/s]\u001b[A\n",
      " 71%|███████   | 1781/2500 [00:10<00:05, 122.02it/s]\u001b[A\n",
      " 72%|███████▏  | 1796/2500 [00:10<00:05, 129.02it/s]\u001b[A\n",
      " 73%|███████▎  | 1813/2500 [00:10<00:05, 136.96it/s]\u001b[A\n",
      " 73%|███████▎  | 1828/2500 [00:10<00:04, 137.83it/s]\u001b[A\n",
      " 74%|███████▎  | 1842/2500 [00:10<00:04, 136.54it/s]\u001b[A\n",
      " 74%|███████▍  | 1856/2500 [00:10<00:04, 130.46it/s]\u001b[A\n",
      " 75%|███████▍  | 1871/2500 [00:10<00:04, 135.15it/s]\u001b[A\n",
      " 75%|███████▌  | 1886/2500 [00:10<00:04, 136.64it/s]\u001b[A\n",
      " 76%|███████▌  | 1902/2500 [00:11<00:04, 141.91it/s]\u001b[A\n",
      " 77%|███████▋  | 1917/2500 [00:11<00:04, 137.43it/s]\u001b[A\n",
      " 77%|███████▋  | 1931/2500 [00:11<00:04, 133.28it/s]\u001b[A\n",
      " 78%|███████▊  | 1945/2500 [00:11<00:04, 130.56it/s]\u001b[A\n",
      " 78%|███████▊  | 1959/2500 [00:11<00:04, 128.15it/s]\u001b[A\n",
      " 79%|███████▉  | 1972/2500 [00:11<00:04, 125.30it/s]\u001b[A\n",
      " 79%|███████▉  | 1985/2500 [00:11<00:04, 124.91it/s]\u001b[A\n",
      " 80%|███████▉  | 1998/2500 [00:11<00:03, 125.80it/s]\u001b[A\n",
      " 80%|████████  | 2011/2500 [00:11<00:04, 120.87it/s]\u001b[A\n",
      " 81%|████████  | 2024/2500 [00:12<00:04, 117.41it/s]\u001b[A\n",
      " 81%|████████▏ | 2036/2500 [00:12<00:03, 116.83it/s]\u001b[A\n",
      " 82%|████████▏ | 2049/2500 [00:12<00:03, 118.21it/s]\u001b[A\n",
      " 83%|████████▎ | 2063/2500 [00:12<00:03, 121.77it/s]\u001b[A\n",
      " 83%|████████▎ | 2076/2500 [00:12<00:03, 122.32it/s]\u001b[A\n",
      " 84%|████████▎ | 2090/2500 [00:12<00:03, 124.25it/s]\u001b[A\n",
      " 84%|████████▍ | 2104/2500 [00:12<00:03, 128.20it/s]\u001b[A\n",
      " 85%|████████▍ | 2119/2500 [00:12<00:02, 133.50it/s]\u001b[A\n",
      " 85%|████████▌ | 2133/2500 [00:12<00:02, 134.30it/s]\u001b[A\n",
      " 86%|████████▌ | 2147/2500 [00:12<00:02, 133.09it/s]\u001b[A\n",
      " 86%|████████▋ | 2162/2500 [00:13<00:02, 135.80it/s]\u001b[A\n",
      " 87%|████████▋ | 2177/2500 [00:13<00:02, 138.24it/s]\u001b[A\n",
      " 88%|████████▊ | 2192/2500 [00:13<00:02, 140.07it/s]\u001b[A\n",
      " 88%|████████▊ | 2207/2500 [00:13<00:02, 132.70it/s]\u001b[A\n",
      " 89%|████████▉ | 2221/2500 [00:13<00:02, 128.29it/s]\u001b[A\n",
      " 89%|████████▉ | 2234/2500 [00:13<00:02, 125.50it/s]\u001b[A\n",
      " 90%|████████▉ | 2247/2500 [00:13<00:02, 120.02it/s]\u001b[A\n",
      " 90%|█████████ | 2260/2500 [00:13<00:02, 113.85it/s]\u001b[A\n",
      " 91%|█████████ | 2272/2500 [00:13<00:02, 110.88it/s]\u001b[A\n",
      " 91%|█████████▏| 2284/2500 [00:14<00:01, 110.48it/s]\u001b[A\n",
      " 92%|█████████▏| 2296/2500 [00:14<00:01, 109.68it/s]\u001b[A\n",
      " 92%|█████████▏| 2308/2500 [00:14<00:01, 111.46it/s]\u001b[A\n",
      " 93%|█████████▎| 2320/2500 [00:14<00:01, 111.60it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 2332/2500 [00:14<00:01, 108.29it/s]\u001b[A\n",
      " 94%|█████████▎| 2343/2500 [00:14<00:01, 101.72it/s]\u001b[A\n",
      " 94%|█████████▍| 2354/2500 [00:14<00:01, 101.75it/s]\u001b[A\n",
      " 95%|█████████▍| 2365/2500 [00:14<00:01, 99.19it/s] \u001b[A\n",
      " 95%|█████████▌| 2376/2500 [00:15<00:01, 96.61it/s]\u001b[A\n",
      " 95%|█████████▌| 2387/2500 [00:15<00:01, 97.51it/s]\u001b[A\n",
      " 96%|█████████▌| 2398/2500 [00:15<00:01, 99.24it/s]\u001b[A\n",
      " 96%|█████████▋| 2409/2500 [00:15<00:00, 100.88it/s]\u001b[A\n",
      " 97%|█████████▋| 2420/2500 [00:15<00:00, 100.71it/s]\u001b[A\n",
      " 97%|█████████▋| 2431/2500 [00:15<00:00, 103.18it/s]\u001b[A\n",
      " 98%|█████████▊| 2443/2500 [00:15<00:00, 105.34it/s]\u001b[A\n",
      " 98%|█████████▊| 2455/2500 [00:15<00:00, 107.85it/s]\u001b[A\n",
      " 99%|█████████▊| 2467/2500 [00:15<00:00, 109.30it/s]\u001b[A\n",
      " 99%|█████████▉| 2479/2500 [00:15<00:00, 110.24it/s]\u001b[A\n",
      "100%|█████████▉| 2491/2500 [00:16<00:00, 108.72it/s]\u001b[A\n",
      "100%|██████████| 2500/2500 [00:16<00:00, 154.51it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "colorizer.load_generator('g_weights_50.h5')  # generator params with 50-epoch training\n",
    "data_generator = CifarGenerator(img_dir=TEST_DIR, batch_size=4, is_training=False)\n",
    "\n",
    "check_files = ['img-3965-airplane.png', 'img-6109-bird.png', 'img-8077-cat.png', 'img-5399-frog.png',\n",
    "               'img-8629-horse.png', 'img-2663-horse.png', 'img-3786-horse.png', 'img-3473-bird.png', \n",
    "               'img-8485-airplane.png','img-950-cat.png', 'img-8953-truck.png', 'img-5628-ship.png', \n",
    "               'img-3064-automobile.png', 'img-4683-deer.png', 'img-9229-automobile.png', 'img-7525-ship.png']\n",
    "\n",
    "for images_B, f_names in tqdm(data_generator):\n",
    "    for index in range(fake_A.shape[0]):\n",
    "        if f_names[index] in check_files:\n",
    "            fake_A = colorizer.generator.predict(images_B)\n",
    "            save_single('./cgan_gen', f_names[index], fake_A[index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate(image1, image2):\n",
    "    g = image1.histogram()\n",
    "    s = image2.histogram()\n",
    "    assert len(g) == len(s), \"error\"\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for index in range(0, len(g)):\n",
    "        if g[index] != s[index]:\n",
    "            data.append(1 - abs(g[index] - s[index]) / max(g[index], s[index]))\n",
    "        else:\n",
    "            data.append(1)\n",
    "\n",
    "    return sum(data) / len(g)\n",
    "\n",
    "def split_image(image, part_size):\n",
    "    pw, ph = part_size\n",
    "    w, h = image.size\n",
    "\n",
    "    sub_image_list = []\n",
    "\n",
    "    assert w % pw == h % ph == 0, \"error\"\n",
    "\n",
    "    for i in range(0, w, pw):\n",
    "        for j in range(0, h, ph):\n",
    "            sub_image = image.crop((i, j, i + pw, j + ph)).copy()\n",
    "            sub_image_list.append(sub_image)\n",
    "\n",
    "    return sub_image_list\n",
    "\n",
    "size = (32, 32)\n",
    "part_size = (8, 8)\n",
    "\n",
    "def get_his_sim(ori_img, fake_img):\n",
    "    img1 = ori_img.resize(size).convert(\"RGB\")\n",
    "    sub_image_1 = split_image(img1, part_size)\n",
    "    \n",
    "    img2 = fake_img.resize(size).convert(\"RGB\")\n",
    "    sub_image_2 = split_image(img2, part_size)\n",
    "    \n",
    "    sub_data = 0\n",
    "    for im1, im2 in zip(sub_image_1, sub_image_2):\n",
    "        sub_data += calculate(im1, im2)\n",
    "    \n",
    "    x = size[0] / part_size[0]\n",
    "    y = size[1] / part_size[1]\n",
    "\n",
    "    sim = round((sub_data / (x * y)), 6)\n",
    "\n",
    "def L1(yhat, y):\n",
    "    loss = np.sum(np.abs(y - yhat))\n",
    "    return loss\n",
    " \n",
    "def L2(yhat, y):\n",
    "    loss =np.sum(np.power((y - yhat), 2))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 227759.44it/s]\n",
      "  0%|          | 0/10000 [00:00<?, ?it/s]/usr/local/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:36: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/usr/local/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:39: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "/usr/local/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:40: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "100%|██████████| 10000/10000 [01:42<00:00, 98.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1877839.9333333424\n",
      "262301.55083429546\n",
      "187.78399333333425\n",
      "26.230155083429544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# compare with original\n",
    "batch_size = 1\n",
    "colorizer.load_generator('g_weights_50.h5')  # generator params with 50-epoch training\n",
    "data_generator = CifarGenerator(img_dir=TEST_DIR, batch_size=batch_size)\n",
    "\n",
    "his_sum = 0\n",
    "l1_sum = 0\n",
    "l2_sum = 0\n",
    "\n",
    "for batch_cnt, (images_A, images_B) in enumerate(tqdm(data_generator)):\n",
    "    fake_A = colorizer.generator.predict(images_B)[0]\n",
    "    \n",
    "    image_A = np.uint8((np.array(images_A[0]) * 0.5 + 0.5) * 255)\n",
    "    image_fake = np.uint8((np.array(fake_A) * 0.5 + 0.5) * 255)\n",
    "    \n",
    "    # get histogram similarity\n",
    "    # ori_img = Image.fromarray(image_A)\n",
    "    # fake_img = Image.fromarray(image_fake)\n",
    "    # sim = get_his_sim(ori_img, fake_img)\n",
    "    # his_sum += sim\n",
    "    \n",
    "    loss_l1 = L1(image_A / 255, image_fake / 255)\n",
    "    loss_l2 = L2(image_A / 255, image_fake / 255)\n",
    "    \n",
    "    l1_sum += loss_l1\n",
    "    l2_sum += loss_l2\n",
    "    \n",
    "\n",
    "print(l1_sum)\n",
    "print(l2_sum)\n",
    "print(l1_sum / len(data_generator))    \n",
    "print(l2_sum / len(data_generator))    \n",
    "# print(sim_sum / len(data_generator))    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
